# LLM PROJECT
Building an LLM from scratch (this is the first demo) speed of outputs depend on computing power of your system

Uses Cuda as the device, if cuda is not available CPU is used.

There are mainly three files;

1) Bigram- This file holds the bigram model code, uses torch and nn module,
2) GPT- This holds the code for basic GPT transformer architecture, the encoders, decoder more or less same as our bigram model
3) Torch_examples- This has some of the functions from the torch module illustrated (for learning purposes)
